{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two-spiral problem is shown below.  \n",
    "--------------------------\n",
    "\n",
    "1. Randomly decompose the two-spiral problem into 4 sub-problems and use a Min-Max Modular neural network to solve the two-spiral problem. Each sub-module is a multilayer quadratic perceptron (MLQP) network with one hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# randomly decompose the problem into 4 sub-problem \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "# load training data\n",
    "train_path='data/two_spiral_train.txt'\n",
    "train_data=np.loadtxt(train_path)\n",
    "X=train_data[:,:2]\n",
    "Y=train_data[:,-1]\n",
    "print X.shape,Y.shape\n",
    "\n",
    "# randomly decompose into 4 sub\n",
    "# seperate 0,1 classes\n",
    "X_0=X[Y==0,:]\n",
    "X_1=X[Y==1,:]\n",
    "print X_0.shape\n",
    "# each seperate to two balance sub randomlu\n",
    "index = np.random.permutation(X_0.shape[0])\n",
    "X_0_0=X_0[index[:X_0.shape[0]/2],:]\n",
    "X_0_1=X_0[index[X_0.shape[0]/2:],:]\n",
    "\n",
    "index = np.random.permutation(X_1.shape[0])\n",
    "X_1_0=X_1[index[:X_1.shape[0]/2],:]\n",
    "X_1_1=X_1[index[X_1.shape[0]/2:],:]\n",
    "\n",
    "Xd={0:X_0_0, 1:X_0_1,2:X_1_0, 3:X_1_1}\n",
    "Yd={0:np.zeros((X_0_0.shape[0],1)),1:np.zeros((X_0_1.shape[0],1)),2:np.ones((X_1_0.shape[0],1)),3:np.ones((X_1_1.shape[0],1))}\n",
    "\n",
    "# plot\n",
    "plt.scatter(X[:,0],X[:,1],c=Y)\n",
    "plt.axis('equal')\n",
    "# \n",
    "plt.figure()\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    if i<2:\n",
    "        plt.plot(Xd[i][:,0],Xd[i][:,1],'r.')\n",
    "    else:\n",
    "        plt.plot(Xd[i][:,0],Xd[i][:,1],'b.')\n",
    "    plt.axis('equal')\n",
    "#\n",
    "plt.figure()\n",
    "plt.subplot(141)\n",
    "plt.plot(X_0_0[:,0],X_0_0[:,1],'r.')\n",
    "plt.plot(X_1_0[:,0],X_1_0[:,1],'b.')\n",
    "plt.axis('equal')\n",
    "plt.subplot(142)\n",
    "plt.plot(X_0_1[:,0],X_0_1[:,1],'r.')\n",
    "plt.plot(X_1_1[:,0],X_1_1[:,1],'b.')\n",
    "plt.axis('equal')\n",
    "plt.subplot(143)\n",
    "plt.plot(X_1_0[:,0],X_1_0[:,1],'b.')\n",
    "plt.plot(X_0_1[:,0],X_0_1[:,1],'r.')\n",
    "plt.axis('equal')\n",
    "plt.subplot(144)\n",
    "plt.plot(X_1_1[:,0],X_1_1[:,1],'b.')\n",
    "plt.plot(X_0_0[:,0],X_0_0[:,1],'r.')\n",
    "plt.axis('equal')\n",
    "\n",
    "\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "#  Min-Max Modular neural network \n",
    "def train(X,Y,flag):\n",
    "    mlqps = MLQPs(n_input=X.shape[1],n_output=Y.shape[1],n_hidden=10) # the number of hidden unitss can be set to 10\n",
    "    mlqps.train(X,Y,learning_rate=learning_rate,training_epoches=1e4, display_epoch=1000)\n",
    "    print flag\n",
    "\n",
    "for i in range(4):\n",
    "    train(X[i],Y[i],i)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import os, time, random\n",
    "\n",
    "def long_time_task(name):\n",
    "    print('Run task %s (%s)...' % (name, os.getpid()))\n",
    "    start = time.time()\n",
    "    time.sleep(random.random() * 3)\n",
    "    end = time.time()\n",
    "    print('Task %s runs %0.2f seconds.' % (name, (end - start)))\n",
    "\n",
    "if __name__=='__main__':\n",
    "    print('Parent process %s.' % os.getpid())\n",
    "    p = Pool(4)\n",
    "    for i in range(5):\n",
    "        p.apply_async(long_time_task, args=(i,))\n",
    "    print('Waiting for all subprocesses done...')\n",
    "    p.close()\n",
    "    p.join()\n",
    "    print('All subprocesses done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Plot the decision boundaries of each trained sub-module and the Min-Max Modular neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compare the training time and generalization performance of the above Min-Max Modular neural network and the single MLQP networks trained in Homework Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#------------ on-line learning for MLQPs -----------#\n",
    "import numpy as np\n",
    "\n",
    "# quadratic perceptron\n",
    "class QP:\n",
    "    def __init__(self,n_input,n_output):\n",
    "        self.U=np.random.uniform(0,1,(n_input,n_output))\n",
    "        self.V=np.random.uniform(0,1,(n_input,n_output))\n",
    "        self.b=np.ones((1,n_output))\n",
    "        self.one=np.ones(shape=self.b.shape)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # x: the input data (1 x n_input)\n",
    "        t=np.dot(x**2,self.U)+np.dot(x,self.V)+self.b\n",
    "        self.y=1/(1+np.exp(-t))\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self,x,gy,lr):\n",
    "        # gy: the gradient of y (1 x n_output) lr:learning_rate, for updating the parameterse\n",
    "        gt=gy*self.y*(self.one-self.y) \n",
    "\n",
    "        gx=np.dot(gt,self.U.T)*2*x + np.dot(gt,self.V.T)\n",
    "        gU=np.dot((x**2).T,gt)\n",
    "        gV=np.dot(x.T,gt)\n",
    "        gb=gt\n",
    "        \n",
    "        # update parameteres\n",
    "        self.U=self.U-lr*gU\n",
    "        self.V=self.V-lr*gV\n",
    "        self.b=self.b-lr*gb\n",
    "        \n",
    "        return gx\n",
    "\n",
    "# MLQPs\n",
    "class MLQPs:\n",
    "    def __init__(self,n_input,n_output,n_hidden=10):\n",
    "        self.qp1=QP(n_input,n_hidden)\n",
    "        self.qp2=QP(n_hidden,n_output)\n",
    "\n",
    "    def forward(self,x):\n",
    "        h=self.qp1.forward(x)\n",
    "        pred=self.qp2.forward(h)\n",
    "        return h,pred \n",
    "    \n",
    "    def train(self,X,Y,learning_rate=0.05,training_epoches=1500,display_epoch=500):\n",
    "        for epoch in np.arange(training_epoches):\n",
    "            for i in range(X.shape[0]):\n",
    "                x = X[i:(i+1),:] # on-line. training 1 instance each loop.\n",
    "                y = Y[i:(i+1),:]\n",
    "                # forward\n",
    "                h,pred = self.forward(x)\n",
    "                # L2 \n",
    "                diff = pred-y\n",
    "                cost = np.linalg.norm(diff)\n",
    "                # backward\n",
    "                # out layer\n",
    "                gh = self.qp2.backward(h,diff,learning_rate)\n",
    "                # hidden layer\n",
    "                self.qp1.backward(x,gh,learning_rate)\n",
    "            if cost<0.02:\n",
    "                print 'Finished!  epoch: %d' % epoch,'cost: %f' % cost\n",
    "                return 0\n",
    "            if epoch % display_epoch == 0:\n",
    "                print 'epoch: %d' % epoch,'cost: %f' % cost\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    X_sub = X[index[i*bz:(i+1)*bz],:]\n",
    "    Y_sub = Y[index[i*bz:(i+1)*bz]]\n",
    "    plt.scatter(X_sub[:,0],X_sub[:,1],c=Y_sub)\n",
    "plt.axis('equal')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
